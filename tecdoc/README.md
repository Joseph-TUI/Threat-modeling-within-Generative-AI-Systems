<p align="center">
	<a href="https://github.com/Joseph-TUI/Threat-modeling-within-Generative-AI-Systems/blob/main/README.md">
		<img align="center" alt="Threat modeling-Security Practices" src="/Pic/main.JPG" height="175">
	</a>
</p>

# Overview

Using a variety of test AI-generated chatbot applications, the three main assaults that were discovered are.	


| Attacks	| Description	|
|---	|---	|
| [Adversarial Attacks](https://ieeexplore.ieee.org/document/9256597)	| Attackers can craft input data in a way that subtly perturbs the input while remaining imperceptible to humans but causes the model to generate incorrect or harmful results.	|
| [Model Tampering](https://ieeexplore.ieee.org/document/10169568)	| This could involve modifying model parameters, weights, or architecture to induce biases, generate inappropriate content, or compromise the integrity of the generated outputs. |
| [Output Manipulation](https://ieeexplore.ieee.org/document/10352982)	| Attackers may seek to manipulate the generated content post-production, either to inject malicious content or to deceive users relying on the authenticity of the generated output.|

Impact of the STRIDE model on three widely recognized chatbots, examining potential attacks.

<p align="center">
	<a href="https://github.com/Joseph-TUI/Threat-modeling-within-Generative-AI-Systems/blob/main/README.md">
		<img align="center" alt="Threat modeling-Security Practices" src="/Pic/Figure-1.JPG" height="400">
	</a>
</p>
