<p align="center">
	<a href="https://github.com/Joseph-TUI/Threat-modeling-within-Generative-AI-Systems/blob/main/README.md">
		<img align="center" alt="Threat modeling-Security Practices" src="/Pic/main.JPG" height="175">
	</a>
</p>

# Overview

Using a variety of test AI-generated chatbot applications, the three main assaults that were discovered are.	

•	Adversarial Attacks: Attackers can craft input data in a way that subtly perturbs the input while remaining imperceptible to humans but causes the model to generate incorrect or harmful results.

•	Model Tampering: This could involve modifying model parameters, weights, or architecture to induce biases, generate inappropriate content, or compromise the integrity of the generated outputs.

•	Output Manipulation: Attackers may seek to manipulate the generated content post-production, either to inject malicious content or to deceive users relying on the authenticity of the generated output. 

Impact of the STRIDE model on three widely recognized chatbots, examining potential attacks.

<p align="center">
	<a href="https://github.com/Joseph-TUI/Threat-modeling-within-Generative-AI-Systems/blob/main/README.md">
		<img align="center" alt="Threat modeling-Security Practices" src="/Pic/Figure-1.JPG" height="400">
	</a>
</p>
